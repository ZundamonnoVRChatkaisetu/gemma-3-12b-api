from fastapi import APIRouter, Depends, HTTPException, status, Request, Response
from fastapi.responses import StreamingResponse
from typing import Dict, List, Optional, Any
import logging
import time
import json
import uuid

from ..models.chat_model import get_chat_model, Message as ChatMessage
from ..models.model_factory import get_model, get_tokenizer
from ..models.files_assistant import get_files_assistant
from ..models.smart_assistant import get_smart_assistant
from ..models.schemas import ChatCompletionRequest, ChatCompletionResponse, Message
from ..core.dependencies import check_rate_limit
from ..core.database import get_memory_setting, get_session, create_session, add_message
from ..core.database import store_user_memory, get_user_memory, delete_user_memory
from .user_memory import detect_memory_intent, extract_key_value_from_memory_text

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post(
    "/chat/completions", 
    response_model=ChatCompletionResponse,
    summary="ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹",
    description="ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™",
    dependencies=[Depends(check_rate_limit)],
)
async def chat_completion(request: Request, data: ChatCompletionRequest):
    """
    ãƒãƒ£ãƒƒãƒˆå¿œç­”ç”Ÿæˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    * messages: ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆï¼ˆæœ€å¾Œã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ã‚‚ã®ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼‰
    * max_tokens: ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    * temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé«˜ã„ã»ã©ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
    * top_p: top-p ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    * top_k: top-k ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    * stream: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆã‚’è¡Œã†ã‹ã©ã†ã‹
    * session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³IDï¼ˆãƒ¡ãƒ¢ãƒªæ©Ÿèƒ½ä½¿ç”¨æ™‚ã€æŒ‡å®šã—ãªã„å ´åˆã¯æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒä½œæˆã•ã‚Œã¾ã™ï¼‰
    """
    start_time = time.time()
    
    try:
        chat_model = get_chat_model()
        model = get_model()
        tokenizer = get_tokenizer()
        smart_assistant = get_smart_assistant()
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’ChatMessageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        chat_messages = [
            ChatMessage(role=msg.role, content=msg.content)
            for msg in data.messages
        ]
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã®å–å¾—ï¼ˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰ã€ã¾ãŸã¯æ–°è¦ç”Ÿæˆï¼‰
        session_id = data.session_id if hasattr(data, 'session_id') and data.session_id else str(uuid.uuid4())
        
        # ãƒ¡ãƒ¢ãƒªæ©Ÿèƒ½ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ç¢ºèª
        memory_enabled_str = get_memory_setting("memory_enabled")
        memory_enabled = memory_enabled_str == "true" if memory_enabled_str else True
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©è¨˜æ†¶æ©Ÿèƒ½ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ç¢ºèª
        user_memory_enabled_str = get_memory_setting("user_memory_enabled")
        user_memory_enabled = user_memory_enabled_str == "true" if user_memory_enabled_str else True
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã€ãªã‘ã‚Œã°ä½œæˆ
        if memory_enabled:
            session = get_session(session_id)
            if not session:
                create_session(session_id)
        
        # æœ€å¾Œã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
        latest_user_message = data.messages[-1].content if data.messages and data.messages[-1].role == "user" else ""
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©è¨˜æ†¶ã®æ„å›³ã‚’æ¤œå‡ºï¼ˆã€Œã€‡ã€‡ã‚’è¦šãˆã¦ã€ãªã©ï¼‰
        if latest_user_message and user_memory_enabled:
            is_memory_op, op_type, content = detect_memory_intent(latest_user_message)
            
            if is_memory_op:
                # è¨˜æ†¶æ“ä½œã‚’å®Ÿè¡Œ
                response_text = ""
                
                if op_type == "store":
                    # è¨˜æ†¶ã‚’ä¿å­˜
                    key, value = extract_key_value_from_memory_text(content)
                    store_user_memory(key, value, session_id)
                    response_text = f"ã€Œ{key}ã€ã‚’è¨˜æ†¶ã—ã¾ã—ãŸã€‚å¿…è¦ãªã¨ãã«ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚"
                
                elif op_type == "retrieve":
                    # è¨˜æ†¶ã‚’å–å¾—
                    memory = get_user_memory(content)
                    if memory:
                        response_text = f"ã€Œ{content}ã€ã«ã¤ã„ã¦ã®è¨˜æ†¶ã§ã™: {memory['value']}"
                    else:
                        response_text = f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã€Œ{content}ã€ã«ã¤ã„ã¦ã®è¨˜æ†¶ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
                
                elif op_type == "forget":
                    # è¨˜æ†¶ã‚’å‰Šé™¤
                    success = delete_user_memory(content)
                    if success:
                        response_text = f"ã€Œ{content}ã€ã«ã¤ã„ã¦ã®è¨˜æ†¶ã‚’å¿˜ã‚Œã¾ã—ãŸã€‚"
                    else:
                        response_text = f"ã€Œ{content}ã€ã«ã¤ã„ã¦ã®è¨˜æ†¶ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                
                # ãƒ¡ãƒ¢ãƒªæ©Ÿèƒ½ãŒæœ‰åŠ¹ãªå ´åˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”ã‚’ä¿å­˜
                if memory_enabled:
                    add_message(session_id, "user", latest_user_message)
                    add_message(session_id, "assistant", response_text)
                
                # å¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
                response = ChatCompletionResponse(
                    message=Message(
                        role="assistant",
                        content=response_text
                    ),
                    usage={
                        "prompt_tokens": 0,
                        "completion_tokens": 0,
                        "total_tokens": 0,
                        "time_seconds": round(time.time() - start_time, 2),
                    },
                    session_id=session_id
                )
                
                return response
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã®æ„å›³ã‚’æ¤œå‡º
        if latest_user_message:
            files_assistant = get_files_assistant()
            is_file_op, op_type, op_params = files_assistant.detect_file_operation(latest_user_message)
            
            if is_file_op:
                # ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã‚’å®Ÿè¡Œ
                result = files_assistant.execute_file_operation(op_type, op_params)
                
                # æ“ä½œçµæœã«åŸºã¥ã„ã¦å¿œç­”ã‚’ç”Ÿæˆ
                if result.get("success", False):
                    if op_type == "list_files":
                        files = result.get("files", [])
                        current_dir = result.get("current_dir", "")
                        
                        files_text = ""
                        for file in files:
                            file_type = "ğŸ“ " if file.get("is_dir") else "ğŸ“„ "
                            size_info = f" ({file.get('size', 0)} bytes)" if not file.get("is_dir") else ""
                            files_text += f"{file_type}{file.get('name')}{size_info}\n"
                        
                        response_text = f"## ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {current_dir or '/'}\n\n{files_text}"
                        
                    elif op_type == "read_file":
                        content = result.get("content", "")
                        path = result.get("path", "")
                        
                        response_text = f"## ãƒ•ã‚¡ã‚¤ãƒ«: {path}\n\n```\n{content}\n```"
                        
                    else:
                        response_text = f"ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œãŒå®Œäº†ã—ã¾ã—ãŸ: {result.get('message', '')}"
                else:
                    response_text = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {result.get('message', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}"
                
                # ãƒ¡ãƒ¢ãƒªæ©Ÿèƒ½ãŒæœ‰åŠ¹ãªå ´åˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”ã‚’ä¿å­˜
                if memory_enabled:
                    add_message(session_id, "user", latest_user_message)
                    add_message(session_id, "assistant", response_text)
                
                # å¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
                response = ChatCompletionResponse(
                    message=Message(
                        role="assistant",
                        content=response_text
                    ),
                    usage={
                        "prompt_tokens": 0,
                        "completion_tokens": 0,
                        "total_tokens": 0,
                        "time_seconds": round(time.time() - start_time, 2),
                    },
                    session_id=session_id
                )
                
                return response
            
            # Webæ¤œç´¢ã®æ„å›³ã‚’æ¤œå‡º
            is_web_search, search_query = smart_assistant.detect_web_search_intent(latest_user_message)
            if is_web_search and search_query:
                # Webæ¤œç´¢çµæœã‚’å«ã‚ãŸå¿œç­”ã‚’ç”Ÿæˆ
                response_text = smart_assistant.enhance_response_with_search(search_query, latest_user_message)
                
                # ãƒ¡ãƒ¢ãƒªæ©Ÿèƒ½ãŒæœ‰åŠ¹ãªå ´åˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”ã‚’ä¿å­˜
                if memory_enabled:
                    add_message(session_id, "user", latest_user_message)
                    add_message(session_id, "assistant", response_text)
                
                # å¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
                response = ChatCompletionResponse(
                    message=Message(
                        role="assistant",
                        content=response_text
                    ),
                    usage={
                        "prompt_tokens": 0,
                        "completion_tokens": 0,
                        "total_tokens": 0,
                        "time_seconds": round(time.time() - start_time, 2),
                    },
                    session_id=session_id
                )
                
                return response
            
            # GitHubæ“ä½œã®æ„å›³ã‚’æ¤œå‡º
            is_github_op, op_type, op_params = smart_assistant.detect_github_operation_intent(latest_user_message)
            if is_github_op:
                # GitHubæ“ä½œã‚’å®Ÿè¡Œ
                result = smart_assistant.perform_github_operation(op_type, op_params)
                
                # æ“ä½œçµæœã‚’æ•´å½¢
                response_text = smart_assistant.format_github_operation_result(op_type, result)
                
                # ãƒ¡ãƒ¢ãƒªæ©Ÿèƒ½ãŒæœ‰åŠ¹ãªå ´åˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”ã‚’ä¿å­˜
                if memory_enabled:
                    add_message(session_id, "user", latest_user_message)
                    add_message(session_id, "assistant", response_text)
                
                # å¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
                response = ChatCompletionResponse(
                    message=Message(
                        role="assistant",
                        content=response_text
                    ),
                    usage={
                        "prompt_tokens": 0,
                        "completion_tokens": 0,
                        "total_tokens": 0,
                        "time_seconds": round(time.time() - start_time, 2),
                    },
                    session_id=session_id
                )
                
                return response
        
        if data.stream:
            async def streaming_generator():
                try:
                    prompt = chat_model.format_prompt(chat_messages, session_id if memory_enabled else None)
                    for text_chunk in model.generate_text(
                        prompt=prompt,
                        max_tokens=data.max_tokens,
                        temperature=data.temperature,
                        top_p=data.top_p,
                        top_k=data.top_k,
                        stream=True,
                    ):
                        yield f"data: {text_chunk}\n\n"
                except Exception as e:
                    logger.error(f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    yield f"data: [ERROR] {str(e)}\n\n"
                finally:
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†å¾Œã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿å­˜
                    if memory_enabled:
                        try:
                            add_message(session_id, "user", latest_user_message)
                            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã®å ´åˆã€å¿œç­”å…¨ä½“ã‚’å–å¾—ã§ããªã„ãŸã‚ã€ä¿å­˜ã¯è¡Œã‚ãªã„
                        except Exception as save_error:
                            logger.error(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(save_error)}")
                    
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°çµ‚äº†ã‚’é€šçŸ¥
                    yield "data: [DONE]\n\n"
            
            return StreamingResponse(
                streaming_generator(),
                media_type="text/event-stream",
            )
        else:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ•´å½¢ã—ã¦ãƒ¢ãƒ‡ãƒ«ã«é€ä¿¡
            prompt = chat_model.format_prompt(chat_messages, session_id if memory_enabled else None)

            # ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©è¨˜æ†¶ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
            if user_memory_enabled:
                try:
                    from ..core.database import get_all_user_memories
                    memories = get_all_user_memories()
                    
                    if memories:
                        memory_text = "ä»¥ä¸‹ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¨˜æ†¶ã¨ã—ã¦ä¿å­˜ã—ãŸæƒ…å ±ã§ã™ï¼š\n"
                        for memory in memories:
                            memory_text += f"ãƒ»{memory['key']}: {memory['value']}\n"
                        memory_text += "\nå¿…è¦ã«å¿œã˜ã¦ä¸Šè¨˜ã®æƒ…å ±ã‚’å‚ç…§ã—ã¦å¿œç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\n"
                        
                        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’è¿½åŠ ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å¾Œã‹ã¤ä¼šè©±å‰ã«é…ç½®ï¼‰
                        if "ã‚ãªãŸã¯å½¹ç«‹ã¤AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚" in prompt:
                            prompt = prompt.replace(
                                "ã‚ãªãŸã¯å½¹ç«‹ã¤AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ä¼šè©±ã‚’å…ƒã«æœ€æ–°ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\n",
                                f"ã‚ãªãŸã¯å½¹ç«‹ã¤AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ä¼šè©±ã‚’å…ƒã«æœ€æ–°ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\n{memory_text}"
                            )
                except Exception as e:
                    logger.warning(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©è¨˜æ†¶ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            
            response_text = model.generate_text(
                prompt=prompt,
                max_tokens=data.max_tokens,
                temperature=data.temperature,
                top_p=data.top_p,
                top_k=data.top_k,
                stream=False,
            )
            
            # ãƒ¡ãƒ¢ãƒªæ©Ÿèƒ½ãŒæœ‰åŠ¹ãªå ´åˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”ã‚’ä¿å­˜
            if memory_enabled:
                add_message(session_id, "user", latest_user_message)
                add_message(session_id, "assistant", response_text)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®è¨ˆç®—ï¼ˆã“ã‚Œã¯æ¨å®šã§ã™ï¼‰
            try:
                input_tokens = len(tokenizer.encode(prompt))
                output_tokens = len(tokenizer.encode(response_text))
            except:
                # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã«å¤±æ•—ã—ãŸå ´åˆã€å˜èªæ•°ã§ä»£ç”¨
                input_tokens = len(prompt.split())
                output_tokens = len(response_text.split())
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ä½œæˆ
            response = ChatCompletionResponse(
                message=Message(
                    role="assistant",
                    content=response_text
                ),
                usage={
                    "prompt_tokens": input_tokens,
                    "completion_tokens": output_tokens,
                    "total_tokens": input_tokens + output_tokens,
                    "time_seconds": round(time.time() - start_time, 2),
                },
                session_id=session_id
            )
            
            return response
    
    except Exception as e:
        logger.error(f"ãƒãƒ£ãƒƒãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ãƒãƒ£ãƒƒãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
        )

@router.post(
    "/chat/new-session",
    response_model=ChatCompletionResponse,
    summary="æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹",
    description="æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™",
    dependencies=[Depends(check_rate_limit)],
)
async def chat_with_new_session(request: Request, data: ChatCompletionRequest):
    """
    æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹
    
    * messages: ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆï¼ˆæœ€å¾Œã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ã‚‚ã®ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼‰
    * max_tokens: ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    * temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé«˜ã„ã»ã©ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
    * top_p: top-p ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    * top_k: top-k ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    * stream: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆã‚’è¡Œã†ã‹ã©ã†ã‹
    * session_title: ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
    """
    start_time = time.time()
    
    try:
        chat_model = get_chat_model()
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’ChatMessageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        chat_messages = [
            ChatMessage(role=msg.role, content=msg.content)
            for msg in data.messages
        ]
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆæŒ‡å®šãŒãªã‘ã‚Œã°ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ç”Ÿæˆï¼‰
        session_title = getattr(data, 'session_title', None)
        if not session_title and chat_messages and chat_messages[0].role == "user":
            # æœ€åˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å…ˆé ­20æ–‡å­—ã‚’ã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦ä½¿ç”¨
            session_title = chat_messages[0].content[:20] + "..."
        
        # æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§å¿œç­”ã‚’ç”Ÿæˆ
        result = chat_model.generate_with_new_session(
            messages=chat_messages,
            title=session_title,
            max_tokens=data.max_tokens,
            temperature=data.temperature,
            top_p=data.top_p,
            top_k=data.top_k,
            stream=data.stream,
        )
        
        if data.stream:
            async def streaming_generator():
                try:
                    for text_chunk in result["response"]:
                        yield f"data: {text_chunk}\n\n"
                except Exception as e:
                    logger.error(f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    yield f"data: [ERROR] {str(e)}\n\n"
                finally:
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’å«ã‚ã¦çµ‚äº†ã‚’é€šçŸ¥
                    yield f"data: [SESSION]{result['session_id']}\n\n"
                    yield "data: [DONE]\n\n"
            
            return StreamingResponse(
                streaming_generator(),
                media_type="text/event-stream",
            )
        else:
            # å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
            response_text = result["response"]
            session_id = result["session_id"]
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®è¨ˆç®—ï¼ˆæ¨å®šï¼‰
            try:
                tokenizer = get_tokenizer()
                prompt = chat_model.format_prompt(chat_messages)
                input_tokens = len(tokenizer.encode(prompt))
                output_tokens = len(tokenizer.encode(response_text))
            except:
                # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã«å¤±æ•—ã—ãŸå ´åˆã€å˜èªæ•°ã§ä»£ç”¨
                prompt = " ".join([msg.content for msg in chat_messages])
                input_tokens = len(prompt.split())
                output_tokens = len(response_text.split())
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ä½œæˆ
            response = ChatCompletionResponse(
                message=Message(
                    role="assistant",
                    content=response_text
                ),
                usage={
                    "prompt_tokens": input_tokens,
                    "completion_tokens": output_tokens,
                    "total_tokens": input_tokens + output_tokens,
                    "time_seconds": round(time.time() - start_time, 2),
                },
                session_id=session_id
            )
            
            return response
    
    except Exception as e:
        logger.error(f"ãƒãƒ£ãƒƒãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ãƒãƒ£ãƒƒãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
        )
