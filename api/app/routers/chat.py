from fastapi import APIRouter, Depends, HTTPException, status, Request, Response
from fastapi.responses import StreamingResponse
from typing import Dict, List, Optional, Any
import logging
import time
import json

from ..models.chat_model import get_chat_model, Message as ChatMessage
from ..models.model_factory import get_model, get_tokenizer
from ..models.files_assistant import get_files_assistant
from ..models.smart_assistant import get_smart_assistant
from ..models.schemas import ChatCompletionRequest, ChatCompletionResponse, Message
from ..core.dependencies import check_rate_limit

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post(
    "/chat/completions", 
    response_model=ChatCompletionResponse,
    summary="ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹",
    description="ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™",
    dependencies=[Depends(check_rate_limit)],
)
async def chat_completion(request: Request, data: ChatCompletionRequest):
    """
    ãƒãƒ£ãƒƒãƒˆå¿œç­”ç”Ÿæˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    * messages: ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆï¼ˆæœ€å¾Œã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ã‚‚ã®ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼‰
    * max_tokens: ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    * temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé«˜ã„ã»ã©ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
    * top_p: top-p ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    * top_k: top-k ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    * stream: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆã‚’è¡Œã†ã‹ã©ã†ã‹
    """
    start_time = time.time()
    
    try:
        chat_model = get_chat_model()
        model = get_model()
        tokenizer = get_tokenizer()
        smart_assistant = get_smart_assistant()
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’ChatMessageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        chat_messages = [
            ChatMessage(role=msg.role, content=msg.content)
            for msg in data.messages
        ]
        
        # æœ€å¾Œã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
        latest_user_message = data.messages[-1].content if data.messages and data.messages[-1].role == "user" else ""
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã®æ„å›³ã‚’æ¤œå‡º
        if latest_user_message:
            files_assistant = get_files_assistant()
            is_file_op, op_type, op_params = files_assistant.detect_file_operation(latest_user_message)
            
            if is_file_op:
                # ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã‚’å®Ÿè¡Œ
                result = files_assistant.execute_file_operation(op_type, op_params)
                
                # æ“ä½œçµæœã«åŸºã¥ã„ã¦å¿œç­”ã‚’ç”Ÿæˆ
                if result.get("success", False):
                    if op_type == "list_files":
                        files = result.get("files", [])
                        current_dir = result.get("current_dir", "")
                        
                        files_text = ""
                        for file in files:
                            file_type = "ğŸ“ " if file.get("is_dir") else "ğŸ“„ "
                            size_info = f" ({file.get('size', 0)} bytes)" if not file.get("is_dir") else ""
                            files_text += f"{file_type}{file.get('name')}{size_info}\n"
                        
                        response_text = f"## ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {current_dir or '/'}\n\n{files_text}"
                        
                    elif op_type == "read_file":
                        content = result.get("content", "")
                        path = result.get("path", "")
                        
                        response_text = f"## ãƒ•ã‚¡ã‚¤ãƒ«: {path}\n\n```\n{content}\n```"
                        
                    else:
                        response_text = f"ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œãŒå®Œäº†ã—ã¾ã—ãŸ: {result.get('message', '')}"
                else:
                    response_text = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {result.get('message', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}"
                
                # å¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
                response = ChatCompletionResponse(
                    message=Message(
                        role="assistant",
                        content=response_text
                    ),
                    usage={
                        "prompt_tokens": 0,
                        "completion_tokens": 0,
                        "total_tokens": 0,
                        "time_seconds": round(time.time() - start_time, 2),
                    }
                )
                
                return response
            
            # Webæ¤œç´¢ã®æ„å›³ã‚’æ¤œå‡º
            is_web_search, search_query = smart_assistant.detect_web_search_intent(latest_user_message)
            if is_web_search and search_query:
                # Webæ¤œç´¢çµæœã‚’å«ã‚ãŸå¿œç­”ã‚’ç”Ÿæˆ
                response_text = smart_assistant.enhance_response_with_search(search_query, latest_user_message)
                
                # å¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
                response = ChatCompletionResponse(
                    message=Message(
                        role="assistant",
                        content=response_text
                    ),
                    usage={
                        "prompt_tokens": 0,
                        "completion_tokens": 0,
                        "total_tokens": 0,
                        "time_seconds": round(time.time() - start_time, 2),
                    }
                )
                
                return response
            
            # GitHubæ“ä½œã®æ„å›³ã‚’æ¤œå‡º
            is_github_op, op_type, op_params = smart_assistant.detect_github_operation_intent(latest_user_message)
            if is_github_op:
                # GitHubæ“ä½œã‚’å®Ÿè¡Œ
                result = smart_assistant.perform_github_operation(op_type, op_params)
                
                # æ“ä½œçµæœã‚’æ•´å½¢
                response_text = smart_assistant.format_github_operation_result(op_type, result)
                
                # å¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
                response = ChatCompletionResponse(
                    message=Message(
                        role="assistant",
                        content=response_text
                    ),
                    usage={
                        "prompt_tokens": 0,
                        "completion_tokens": 0,
                        "total_tokens": 0,
                        "time_seconds": round(time.time() - start_time, 2),
                    }
                )
                
                return response
        
        if data.stream:
            async def streaming_generator():
                try:
                    prompt = chat_model.format_prompt(chat_messages)
                    for text_chunk in model.generate_text(
                        prompt=prompt,
                        max_tokens=data.max_tokens,
                        temperature=data.temperature,
                        top_p=data.top_p,
                        top_k=data.top_k,
                        stream=True,
                    ):
                        yield f"data: {text_chunk}\n\n"
                except Exception as e:
                    logger.error(f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    yield f"data: [ERROR] {str(e)}\n\n"
                finally:
                    yield "data: [DONE]\n\n"
            
            return StreamingResponse(
                streaming_generator(),
                media_type="text/event-stream",
            )
        else:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ•´å½¢ã—ã¦ãƒ¢ãƒ‡ãƒ«ã«é€ä¿¡
            prompt = chat_model.format_prompt(chat_messages)
            response_text = model.generate_text(
                prompt=prompt,
                max_tokens=data.max_tokens,
                temperature=data.temperature,
                top_p=data.top_p,
                top_k=data.top_k,
                stream=False,
            )
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®è¨ˆç®—ï¼ˆã“ã‚Œã¯æ¨å®šã§ã™ï¼‰
            try:
                input_tokens = len(tokenizer.encode(prompt))
                output_tokens = len(tokenizer.encode(response_text))
            except:
                # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã«å¤±æ•—ã—ãŸå ´åˆã€å˜èªæ•°ã§ä»£ç”¨
                input_tokens = len(prompt.split())
                output_tokens = len(response_text.split())
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ä½œæˆ
            response = ChatCompletionResponse(
                message=Message(
                    role="assistant",
                    content=response_text
                ),
                usage={
                    "prompt_tokens": input_tokens,
                    "completion_tokens": output_tokens,
                    "total_tokens": input_tokens + output_tokens,
                    "time_seconds": round(time.time() - start_time, 2),
                }
            )
            
            return response
    
    except Exception as e:
        logger.error(f"ãƒãƒ£ãƒƒãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ãƒãƒ£ãƒƒãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
        )
